---
title: "AI Chat"
description: "Build intelligent conversational experiences with multi-provider AI support"
---

<Frame>
  <img src="/images/534shots_so.png" alt="AI Chat Interface" />
</Frame>

## Overview

Launch includes a production-ready AI chat feature with streaming responses and
chat history persistence. The architecture is modular, making it easy to extend
with additional providers and custom system prompts.

## Prerequisites

- Backend running with AI env vars set
- `EXPO_PUBLIC_API_URL` set in `apps/mobile/.env`

<Tip>
  This feature is gated by the mobile feature registry flag: `ai`. See
  [Feature Registry](/mobile/feature-registry).
</Tip>

## Key Features

<Columns cols={2}>
  <Card title="Provider Abstraction" icon="brain">
    OpenAI is enabled by default. Anthropic support is wired in the backend and
    can be enabled in the mobile model list with a single config change.
  </Card>
  <Card title="Streaming Responses" icon="bolt">
    Real-time streaming for a responsive chat experience with stop generation
    support.
  </Card>
  <Card title="System Prompts" icon="wand-magic-sparkles">
    Predefined personas and custom system prompts to shape AI behavior.
  </Card>
  <Card title="Chat Persistence" icon="database">
    Conversation history stored in PostgreSQL via tRPC procedures.
  </Card>
</Columns>

## Architecture

The AI feature is built with extensibility in mind, separating concerns into distinct layers.

```mermaid
graph TB
    A[Chat Screen] --> B[ChatContext]
    B --> C[useChat Hook]
    B --> D[useChatHistory Hook]
    B --> E[useChatPersistence Hook]
    C --> F[AI Provider Layer]
    F --> G[OpenAI]
    F --> H[Anthropic]
    F --> I[Custom Providers]
    E --> J[tRPC API]
    J --> K[Database]
```

### Core Components

<Card title="AI Provider Abstraction" icon="layer-group">
  **Location**: `lib/ai/providers/` A unified interface for working with
  different AI providers. OpenAI is enabled by default; add Anthropic to the
  providers list to expose Claude models in the UI.
</Card>

<Card title="Custom Hooks" icon="code">
  **Location**: `features/chat/hooks/` Business logic extracted into reusable
  hooks: `useChat` for messaging, `useChatHistory` for history management, and
  `useChatPersistence` for database sync.
</Card>

<Card title="Chat Context" icon="share-nodes">
  **Location**: `features/chat/context/` Global state management for chat
  functionality, reducing prop drilling and simplifying component interactions.
</Card>

## Steps

1. Add API keys to your backend `.env`:

```bash
# OpenAI (required for AI chat)
OPENAI_API_KEY=sk-...

# Anthropic (optional - enable in mobile model list)
ANTHROPIC_API_KEY=sk-ant-...
```

2. Ensure the AI feature flag is enabled:

`apps/mobile/features/feature-registry.tsx` → `featureFlags.ai = true`

3. (Optional) Enable Anthropic models in the mobile picker:

`apps/mobile/lib/ai/providers/index.ts` → add `anthropicProvider` to the
`providers` array.

4. Open the chat screen at `/ai-chat`.

## How It Works

- Mobile streams tokens from the API via `POST /api/ai/stream` (SSE)
- Chat history is persisted via tRPC procedures under `chat.*`
- System prompts are defined in `apps/mobile/lib/ai/prompts/index.ts`

## Customizing System Prompts

System prompts define the AI's persona and behavior:

```typescript
export const systemPrompts = {
  staffEngineerMentor: {
    id: "staff-engineer-mentor",
    name: "Staff Engineer Mentor",
    description: "A senior mentor who teaches through the Socratic method",
    prompt: `You are a senior staff software engineer with 20+ years...`,
  },
  codingAssistant: {
    id: "coding-assistant",
    name: "Coding Assistant",
    description: "A helpful coding assistant",
    prompt: `You are a helpful coding assistant...`,
  },
};
```

## Usage Examples

### Basic Chat Integration

```typescript
import { ChatProvider, useChatContext } from "@/features/chat";

function App() {
  return (
    <ChatProvider>
      <ChatScreen />
    </ChatProvider>
  );
}

function ChatScreen() {
  const { messages, sendMessage, isStreaming } = useChatContext();

  return (
    <View>
      {messages.map((msg) => (
        <ChatBubble key={msg.id} message={msg} />
      ))}
      <Input onSubmit={sendMessage} disabled={isStreaming} />
    </View>
  );
}
```

### Switching Models

```typescript
const { setModel, currentModel } = useChatContext();

// Switch to a different model
setModel("claude-3-5-sonnet");
```

### Using System Prompts

```typescript
import { getSystemPrompt } from "@/lib/ai";

const { setSystemPrompt } = useChatContext();

// Set a predefined persona
const mentorPrompt = getSystemPrompt("staff-engineer-mentor");
setSystemPrompt(mentorPrompt.prompt);
```

## UI Components

### Model Selector

A dropdown component for selecting AI models with provider grouping:

```typescript
<ModelSelect
  value={selectedModel}
  onValueChange={setSelectedModel}
/>
```

### Prompt Suggestions

Contextual prompt cards that adapt to the current system prompt:

```typescript
<PromptSuggestions
  onSelect={handlePromptSelect}
  maxWidth={inputWidth}
/>
```

### Chat History Sheet

A bottom sheet displaying conversation history:

```typescript
<ChatHistorySheet
  isOpen={showHistory}
  onClose={() => setShowHistory(false)}
  onSelectChat={loadChat}
/>
```

## Database Schema

Conversations are persisted using the following schema:

```prisma
model Chat {
  id           String        @id @default(cuid())
  userId       String
  title        String        @db.VarChar(255)
  model        String        @default("gpt-4")
  systemPrompt String?       @db.Text
  createdAt    DateTime      @default(now())
  updatedAt    DateTime      @updatedAt

  user         User          @relation(fields: [userId], references: [id])
  messages     ChatMessage[]
}

model ChatMessage {
  id        String   @id @default(cuid())
  chatId    String
  role      String   @db.VarChar(20)
  content   String   @db.Text
  createdAt DateTime @default(now())

  chat      Chat     @relation(fields: [chatId], references: [id])
}
```

## API Endpoints

The chat feature exposes the following tRPC procedures:

| Procedure          | Type     | Description                        |
| ------------------ | -------- | ---------------------------------- |
| `chat.list`        | Query    | Get all chats for the current user |
| `chat.get`         | Query    | Get a specific chat with messages  |
| `chat.create`      | Mutation | Create a new chat                  |
| `chat.delete`      | Mutation | Delete a chat                      |
| `chat.addMessage`  | Mutation | Add a message to a chat            |
| `chat.updateTitle` | Mutation | Update chat title                  |

## Test Checklist

- Chat screen loads and streams responses
- New chats are saved and appear in history
- Switching models updates the active provider

## Troubleshooting

If streaming fails, verify API keys and server logs. For general issues, start
with [Troubleshooting](/troubleshooting).

## Remove / Disable

To disable AI while you configure providers, set:

`apps/mobile/features/feature-registry.tsx` → `featureFlags.ai = false`

For production removal guidance, see [Removing Features](/essentials/removing-features).

## Troubleshooting

- **Streaming fails**: check `OPENAI_API_KEY` and API logs
- **No models shown**: confirm provider list in `lib/ai/providers/index.ts`

## Next Steps

- [Feature Registry](/mobile/feature-registry)
- [Incident Debugging](/essentials/incident-debugging)














